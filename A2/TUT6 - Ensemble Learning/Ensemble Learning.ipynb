{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02cb7104",
   "metadata": {},
   "source": [
    "# Bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1fce08",
   "metadata": {},
   "source": [
    "### Standalone decision trees vs. Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0bcd4791",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bagged Decision Trees for Classification\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn import preprocessing\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "77902976",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>763</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>764</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>765</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>766</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>767</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>768 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin    BMI  \\\n",
       "0          False    False          False          False    False  False   \n",
       "1          False    False          False          False    False  False   \n",
       "2          False    False          False          False    False  False   \n",
       "3          False    False          False          False    False  False   \n",
       "4          False    False          False          False    False  False   \n",
       "..           ...      ...            ...            ...      ...    ...   \n",
       "763        False    False          False          False    False  False   \n",
       "764        False    False          False          False    False  False   \n",
       "765        False    False          False          False    False  False   \n",
       "766        False    False          False          False    False  False   \n",
       "767        False    False          False          False    False  False   \n",
       "\n",
       "     DiabetesPedigreeFunction    Age  Outcome  \n",
       "0                       False  False    False  \n",
       "1                       False  False    False  \n",
       "2                       False  False    False  \n",
       "3                       False  False    False  \n",
       "4                       False  False    False  \n",
       "..                        ...    ...      ...  \n",
       "763                     False  False    False  \n",
       "764                     False  False    False  \n",
       "765                     False  False    False  \n",
       "766                     False  False    False  \n",
       "767                     False  False    False  \n",
       "\n",
       "[768 rows x 9 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read the data in\n",
    "df = pd.read_csv(\"./Diabetes.csv\")\n",
    "df.isnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc72054e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pregnancies                 False\n",
       "Glucose                     False\n",
       "BloodPressure               False\n",
       "SkinThickness               False\n",
       "Insulin                     False\n",
       "BMI                         False\n",
       "DiabetesPedigreeFunction    False\n",
       "Age                         False\n",
       "Outcome                     False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' is any missing values across columns'''\n",
    "df.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e1eeec5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' count of missing values of the entire dataframe'''\n",
    "df.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6e26328e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the locations\n",
    "X = df.iloc[ : , :-1]\n",
    "y = df.iloc[ : , -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a9e4256c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalize\n",
    "X = StandardScaler().fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f299c215",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model by splitting into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2019)\n",
    "kfold = StratifiedKFold(n_splits=5, random_state=None, shuffle=False)\n",
    "num_trees = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d4fcf47f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree (stand alone) - Train :  0.6678395308543249\n",
      "Decision Tree (stand alone) - Test :  0.6623376623376623\n"
     ]
    }
   ],
   "source": [
    "# Decision Tree with 5 fold cross validation\n",
    "clf_DT = DecisionTreeClassifier(random_state=2019).fit(X_train,y_train)\n",
    "results = cross_val_score(clf_DT, X_train,y_train, cv=kfold)\n",
    "print (\"Decision Tree (stand alone) - Train : \", results.mean())\n",
    "print (\"Decision Tree (stand alone) - Test : \", metrics.accuracy_score(clf_DT.predict(X_test), y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4c17878a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Decision Tree (Bagging) - Train :  0.7378648540583767\n",
      "Decision Tree (Bagging) - Test :  0.7662337662337663\n"
     ]
    }
   ],
   "source": [
    "# Using Bagging Lets build 100 decision tree models and average/majority vote prediction\n",
    "clf_DT_Bag = BaggingClassifier(base_estimator=clf_DT, n_estimators=num_trees, random_state=2019).fit(X_train,y_train)\n",
    "results = cross_val_score(clf_DT_Bag, X_train, y_train, cv=kfold)\n",
    "print (\"\\nDecision Tree (Bagging) - Train : \", results.mean())\n",
    "print (\"Decision Tree (Bagging) - Test : \", metrics.accuracy_score(clf_DT_Bag.predict(X_test), y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c429d9da",
   "metadata": {},
   "source": [
    "### Decision Tree feature importance function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "88d038cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "feature_importance = clf_DT.feature_importances_\n",
    "# make importances relative to max importance\n",
    "feature_importance = 100.0 * (feature_importance / feature_importance.\n",
    "max())\n",
    "sorted_idx = np.argsort(feature_importance)\n",
    "pos = np.arange(sorted_idx.shape[0]) + .5\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.barh(pos, feature_importance[sorted_idx], align='center')\n",
    "plt.yticks(pos, df.columns[sorted_idx])\n",
    "plt.xlabel('Relative Importance')\n",
    "plt.title('Variable Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ef8f50",
   "metadata": {},
   "source": [
    "### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b3dc187d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Random Forest (Bagging) - Train :  0.7395441823270692\n",
      "Random Forest (Bagging) - Test :  0.7987012987012987\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "num_trees = 100\n",
    "kfold = StratifiedKFold(n_splits=5, random_state=None, shuffle=False)\n",
    "clf_RF = RandomForestClassifier(n_estimators=num_trees).fit(X_train, y_train)\n",
    "results = cross_val_score(clf_RF, X_train, y_train, cv=kfold)\n",
    "print (\"\\nRandom Forest (Bagging) - Train : \", results.mean())\n",
    "print (\"Random Forest (Bagging) - Test : \", metrics.accuracy_score(clf_RF.predict(X_test), y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35465b33",
   "metadata": {},
   "source": [
    "### Standalone decision tree vs. Adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "118e2217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bagged Decision Trees for Classification\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Let's use some weak features to build the tree\n",
    "X = df[['Age','Insulin']] # independent variables\n",
    "y = df['Outcome'].values # dependent variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5326cde3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalize\n",
    "X = StandardScaler().fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0f5aa570",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model by splitting into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2019)\n",
    "kfold = StratifiedKFold(n_splits=5, random_state=None, shuffle=False)\n",
    "num_trees = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4eb57eec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree (stand alone) - CV Train : 0.64\n",
      "Decision Tree (stand alone) - Test : 0.64\n",
      "Decision Tree (stand alone) - Test : 0.70\n"
     ]
    }
   ],
   "source": [
    "# Decision Tree with 5 fold cross validation\n",
    "# lets restrict max_depth to 1 to have more impure leaves\n",
    "clf_DT = DecisionTreeClassifier(max_depth=1, random_state=2019).fit(X_train,y_train)\n",
    "results = cross_val_score(clf_DT, X_train,y_train, cv=kfold.split(X_train, y_train))\n",
    "print(\"Decision Tree (stand alone) - CV Train : %.2f\" % results.mean())\n",
    "print(\"Decision Tree (stand alone) - Test : %.2f\" % metrics.accuracy_score(clf_DT.predict(X_train), y_train))\n",
    "print(\"Decision Tree (stand alone) - Test : %.2f\" % metrics.accuracy_score(clf_DT.predict(X_test), y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "15477a0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Decision Tree (AdaBoosting) - CV Train : 0.68\n",
      "Decision Tree (AdaBoosting) - Train : 0.71\n",
      "Decision Tree (AdaBoosting) - Test : 0.79\n"
     ]
    }
   ],
   "source": [
    "#Using Adaptive Boosting of 100 iteration\n",
    "clf_DT_Boost = AdaBoostClassifier(base_estimator=clf_DT, n_estimators=num_trees, learning_rate=0.1, random_state=2019).fit(X_train,y_train)\n",
    "results = cross_val_score(clf_DT_Boost, X_train, y_train, cv=kfold.split(X_train, y_train))\n",
    "print(\"\\nDecision Tree (AdaBoosting) - CV Train : %.2f\" % results.mean())\n",
    "print(\"Decision Tree (AdaBoosting) - Train : %.2f\" % metrics.accuracy_score(clf_DT_Boost.predict(X_train), y_train))\n",
    "print(\"Decision Tree (AdaBoosting) - Test : %.2f\" % metrics.accuracy_score(clf_DT_Boost.predict(X_test), y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a276a0e",
   "metadata": {},
   "source": [
    "# Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1be0dc",
   "metadata": {},
   "source": [
    "### Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9222e8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "# Using Gradient Boosting of 100 iterations\n",
    "clf_GBT = GradientBoostingClassifier(n_estimators=num_trees, learning_rate=0.1, random_state=2019).fit(X_train, y_train)\n",
    "results = cross_val_score(clf_GBT, X_train, y_train, cv=kfold)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954bab07",
   "metadata": {},
   "source": [
    "### xgboost Classifier Using sklearn Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "53677728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "  Downloading xgboost-1.5.2-py3-none-win_amd64.whl (106.6 MB)\n",
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\lib\\site-packages (from xgboost) (1.20.3)\n",
      "Requirement already satisfied: scipy in c:\\programdata\\anaconda3\\lib\\site-packages (from xgboost) (1.7.1)\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-1.5.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cikit-learn (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cikit-learn (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cikit-learn (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cikit-learn (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cikit-learn (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cikit-learn (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cikit-learn (c:\\programdata\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d753f6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "\n",
    "predictors = ['Age','Insulin']\n",
    "target = 'Outcome'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "cd44e178",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "for f in df.columns:\n",
    "    if df[f].dtype=='object':\n",
    "        lbl = preprocessing.LabelEncoder()\n",
    "        lbl.fit(list(df[f].values))\n",
    "        df[f] = lbl.transform(list(df[f].values))\n",
    "df.fillna((-999), inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "944c4119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use some week features to build the tree\n",
    "X = df[['Age','Insulin']] # independent variables\n",
    "y = df['Outcome'].values # dependent variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b27c5dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalize\n",
    "X = StandardScaler().fit_transform(X)\n",
    "\n",
    "# evaluate the model by splitting into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2017)\n",
    "num_rounds = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a6ba5f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = StratifiedKFold(n_splits=5, random_state=None, shuffle=False)\n",
    "clf_XGB = XGBClassifier(n_estimators = num_rounds,objective= 'binary:logistic',seed=2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "9631afe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:22:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[22:22:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[22:22:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[22:22:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[22:22:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[22:22:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "\n",
      "xgBoost - CV Train : 0.69\n",
      "xgBoost - Train : 0.78\n",
      "xgBoost - Test : 0.69\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# use early_stopping_rounds to stop the cv when there is no score imporovement\n",
    "clf_XGB.fit(X_train,y_train, early_stopping_rounds=20, eval_set=[(X_test, y_test)], verbose=False)\n",
    "results = cross_val_score(clf_XGB, X_train,y_train, cv=kfold)\n",
    "print (\"\\nxgBoost - CV Train : %.2f\" % results.mean())\n",
    "print (\"xgBoost - Train : %.2f\" % metrics.accuracy_score(clf_XGB.predict (X_train), y_train))\n",
    "print (\"xgBoost - Test : %.2f\" % metrics.accuracy_score(clf_XGB.predict (X_test), y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f325715",
   "metadata": {},
   "source": [
    "#  Ensemble Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "4ed208ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# set seed for reproducability\n",
    "np.random.seed(2017)\n",
    "import statsmodels.api as sm\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "27b3ab0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mlxtend\n",
      "  Downloading mlxtend-0.19.0-py2.py3-none-any.whl (1.3 MB)\n",
      "Requirement already satisfied: matplotlib>=3.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from mlxtend) (3.4.3)\n",
      "Requirement already satisfied: pandas>=0.24.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from mlxtend) (1.3.4)\n",
      "Requirement already satisfied: scikit-learn>=0.20.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from mlxtend) (0.24.2)\n",
      "Requirement already satisfied: numpy>=1.16.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from mlxtend) (1.20.3)\n",
      "Requirement already satisfied: joblib>=0.13.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from mlxtend) (1.1.0)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from mlxtend) (58.0.4)\n",
      "Requirement already satisfied: scipy>=1.2.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from mlxtend) (1.7.1)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (3.0.4)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (8.4.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (2.8.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (0.10.0)\n",
      "Requirement already satisfied: six in c:\\programdata\\anaconda3\\lib\\site-packages (from cycler>=0.10->matplotlib>=3.0.0->mlxtend) (1.16.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas>=0.24.2->mlxtend) (2021.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn>=0.20.3->mlxtend) (2.2.0)\n",
      "Installing collected packages: mlxtend\n",
      "Successfully installed mlxtend-0.19.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cikit-learn (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cikit-learn (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cikit-learn (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cikit-learn (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cikit-learn (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cikit-learn (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cikit-learn (c:\\programdata\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install mlxtend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "07f5ad75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# currently its available as part of mlxtend and not sklearn\n",
    "from mlxtend.classifier import EnsembleVoteClassifier\n",
    "from sklearn import model_selection\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "import mlxtend\n",
    "# read the data in\n",
    "#df = pd.read_csv(\"Data/Diabetes.csv\")\n",
    "X = df.iloc[:,:8] # independent variables\n",
    "y = df['Outcome'] # dependent variables\n",
    "# evaluate the model by splitting into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "9ed5fafe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5-fold cross validation:\n",
      "\n",
      "Train CV Accuracy: 0.76 (+/- 0.04) [Logistic Regression]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train CV Accuracy: 0.74 (+/- 0.03) [Random Forest]\n",
      "Train CV Accuracy: 0.74 (+/- 0.04) [Support Vector Machine]\n",
      "Train CV Accuracy: 0.70 (+/- 0.05) [KNeighbors]\n",
      "Train CV Accuracy: 0.69 (+/- 0.02) [Decision Tree]\n",
      "Train CV Accuracy: 0.73 (+/- 0.04) [Ada Boost]\n",
      "Train CV Accuracy: 0.73 (+/- 0.03) [Bagging]\n",
      "Train CV Accuracy: 0.75 (+/- 0.04) [Gradient Boosting]\n",
      "Test Accuracy: 0.92 \n"
     ]
    }
   ],
   "source": [
    "LR = LogisticRegression(random_state=2017)\n",
    "RF = RandomForestClassifier(n_estimators = 100, random_state=2017)\n",
    "SVM = SVC(random_state=0, probability=True)\n",
    "KNC = KNeighborsClassifier()\n",
    "DTC = DecisionTreeClassifier()\n",
    "ABC = AdaBoostClassifier(n_estimators = 100)\n",
    "BC = BaggingClassifier(n_estimators = 100)\n",
    "GBC = GradientBoostingClassifier(n_estimators = 100)\n",
    "clfs = []\n",
    "print('5-fold cross validation:\\n')\n",
    "for clf, label in zip([LR, RF, SVM, KNC, DTC, ABC, BC, GBC],\n",
    "                      ['Logistic Regression',\n",
    "                       'Random Forest',\n",
    "                       'Support Vector Machine',\n",
    "                       'KNeighbors',\n",
    "                       'Decision Tree',\n",
    "                       'Ada Boost',\n",
    "                       'Bagging',\n",
    "                       'Gradient Boosting']):\n",
    "    scores = model_selection.cross_val_score(clf, X_train, y_train, cv=5, scoring='accuracy') \n",
    "    print(\"Train CV Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))\n",
    "md = clf.fit(X, y)\n",
    "clfs.append(md)\n",
    "print(\"Test Accuracy: %0.2f \" % (metrics.accuracy_score(clf.predict (X_test), y_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "a2cb925c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5-fold cross validation:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train CV Accuracy: 0.76 (+/- 0.02) [Ensemble Soft Voting]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.94 \n"
     ]
    }
   ],
   "source": [
    "# ### Ensemble Voting\n",
    "clfs = []\n",
    "print('5-fold cross validation:\\n')\n",
    "ECH = EnsembleVoteClassifier(clfs=[LR, RF, GBC], voting='hard')\n",
    "ECS = EnsembleVoteClassifier(clfs=[LR, RF, GBC], voting='soft', weights=[1,1,1])\n",
    "for clf, label in zip([ECH, ECS],\n",
    "                      ['Ensemble Hard Voting',\n",
    "                       'Ensemble Soft Voting']):\n",
    "    scores = model_selection.cross_val_score(clf, X_train, y_train, cv=5, scoring='accuracy')\n",
    "print(\"Train CV Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))\n",
    "md = clf.fit(X, y)\n",
    "clfs.append(md)\n",
    "print(\"Test Accuracy: %0.2f \" % (metrics.accuracy_score(clf.predict (X_test), y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450b9cc1",
   "metadata": {},
   "source": [
    "#### Extract from Mastering Machine Learning with  Python in Six Steps\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
